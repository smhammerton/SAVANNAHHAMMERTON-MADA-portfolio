---
title: "Analysis and Model Fitting Exercise"
output: 
  html_document:
    theme: sandstone
    highlight: zenburn
    toc: false 
---

This is part of an analysis exercise in the Modern Applied Data Analysis course. The repository for the entire exercise can be found [here](https://github.com/smhammerton/SAVANNAHHAMMERTON-MADA-analysis3). 

## Setup

I need to first load the packages we'll be using and load in the data. 

```{r setup, include=FALSE}
#load needed packages 
library(tidyverse) #for data processing 
library(tidymodels) #for model fitting 
library(rsample) #for data splitting 
library(gtsummary) #for summary tables
library(ranger) #for model fitting 
library(rpart) #for model fitting 
library(glmnet) #for model fitting 
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

#load data 
mydata <- readRDS(here::here('files', 'processeddata.rds'))

```

## Feature/Variable Removal

I need to remove all Yes/No versions of variables that also exist as multiple level variables. Then, I need to code the three ordinal factors as ordered, in the proper order. 

```{r feature removal}
#remove yes/no version of multi-level variables - CoughYN, WeaknessYN, CoughYN2, MyalgiaYN
mydata.2 <- mydata %>% 
  select(!c(CoughYN, CoughYN2, WeaknessYN, MyalgiaYN))

#checking to see if ordinal factors are coded as ordered - Weakness, CoughIntensity, Myalgia
is.ordered(mydata.2$Weakness)
is.ordered(mydata.2$CoughIntensity)
is.ordered(mydata.2$Myalgia)

#code ordinal/multi-level factors as ordered factors and check order is None/Mild/Moderate/Severe 
mydata.3 <- mydata.2 %>% 
  mutate(Weakness = ordered(Weakness, levels = c('None', 'Mild', 'Moderate', 'Severe'))) %>% 
  mutate(CoughIntensity = ordered(CoughIntensity, levels = c('None', 'Mild', 'Moderate', 'Severe'))) %>%
  mutate(Myalgia = ordered(Myalgia, levels = c('None', 'Mild', 'Moderate', 'Severe')))

#checking if new variables are coded as ordered 
is.ordered(mydata.3$Weakness)
is.ordered(mydata.3$CoughIntensity)
is.ordered(mydata.3$Myalgia)


```

## Low variance predictors 

I need to remove all binary predictors that have less than <50 entries in one of the categories. To do that, I first need to see what predictors fall into that category. Then I'll remove them from the data set.

```{r low variance predictors}
# create summary table to see what binary predictors have <50 in one category 
mydata.3 %>% 
  select(!c(Weakness, CoughIntensity, Myalgia, BodyTemp)) %>% #removing all non-binary predictors 
  gtsummary::tbl_summary() #create summary table 

# binary predictors with <50 in one category: Loss of Hearing, Blurred Vision
# need to remove those two variables 
mydata.4 <- mydata.3 %>% 
  select(!c(Hearing, Vision))

# Should have 730 observations and 26 variables - checking 
dim(mydata.4) #this looks right! 

# setting to final object name 
mydata.fin <- mydata.4
```

## Data setup 

```{r data setup}
# set seed to fix random numbers 
set.seed(123)

# put 70% of data into training set 
data_split <- initial_split(mydata.fin, 
                            prop = 7/10, #split data set into 70% training, 30% testing
                            strata = BodyTemp) #use bodytemp as stratification 

# create data frames for training and testing sets 
train_data <- training(data_split)
test_data <- testing(data_split)

# create a resample object for the training data; 5x5, stratify on bodytemp 
cv5 <- vfold_cv(train_data, 
                v = 5, 
                repeats = 5, 
                strata = BodyTemp)

# create a recipe for the data and fitting; code categorical variables as dummy variables
temp_rec <- recipes::recipe(BodyTemp ~ ., 
                            data = train_data) %>% 
  step_dummy(all_predictors())

# linear model specification 
linear_reg_lm_spec <-
  linear_reg() %>%
  set_engine('lm')
```

## Null model performance 

```{r null model performance}
#compute the performance of a null model with no predictor information - train data 
null_mod_train <- lm(BodyTemp ~ 1, data = train_data)
summary(null_mod_train)
mean(train_data$BodyTemp) #checking the mean of body temp is the same as the bodytemp coefficient in the null model
#calculate RMSE 
null_mod_train %>% 
  augment(newdata = train_data) %>% 
  rmse(truth = BodyTemp, estimate = .fitted)

#compute the performance of a null model with no predictor information - test data 
null_mod_test <- lm(BodyTemp ~ 1, data = test_data)
summary(null_mod_test)
mean(test_data$BodyTemp) #checking the mean of body temp is the same as the bodytemp coefficient in the null model
#calculate RMSE 
null_rmse <- null_mod_test %>% 
  augment(newdata = test_data) %>% 
  rmse(truth = BodyTemp, estimate = .fitted) %>% 
  mutate(model = 'Null Model')
```

## Model tuning and fitting

1) Define the model specification 
2) Create a workflow 
3) Create a tuning grid and tune the hyperparameters  
4) Autoplot the results 
5) Finalize the workflow
6) Fit the model to the training data 
7) Estimate model performance with RMSE 
8) Make diagnostic plots 

Do these steps for each model (tree, LASSO, random forest)

Finally, look at rmses for all models (including null) together - choose best performing model, fit to test data, then repeat 7 and 8.

### Tree

```{r tree, message=FALSE, cache=TRUE}
# 1) define model spec 
tree_tune_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune()) %>% 
  set_engine('rpart') %>% 
  set_mode('regression')
tree_tune_spec

# 2) workflow 
tree_wflow <- workflow() %>% 
  add_model(tree_tune_spec) %>% 
  add_recipe(temp_rec)


# 3) create tuning grid 
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)

# tune hyperparameters 
tree_res <- tree_wflow %>% 
  tune_grid(resamples = cv5,
            grid = tree_grid,
            metrics = metric_set(rmse),
            control = control_grid(verbose = T))
# 4) autoplot the results 
tree_res %>% autoplot()

# 5) finalize the workflow 
## select best hyperparameter combination 
happy_tree <- tree_res %>% 
  select_best()
happy_tree

## finalize workflow 
happy_tree_wflow <- tree_wflow %>% 
  finalize_workflow(happy_tree)

fit_train_happy_tree <- happy_tree_wflow %>% 
  fit(data = train_data)

# 6) fit the model to training data 
# Use a trained workflow to predict section using test data
predict(fit_train_happy_tree, new_data = train_data)

# include predicted probabilities
aug_train_happy_tree <- augment(fit_train_happy_tree, new_data = train_data)

# 7) estimate model performance with rmse 
tree_rmse <- aug_train_happy_tree %>% 
  rmse(truth = BodyTemp, .pred) %>% 
  mutate(model = 'Regression Tree')

# 8) Make diagnostic plots 
## view decision tree 
extract_fit_engine(fit_train_happy_tree) |> rpart.plot::rpart.plot(digits = 4)

#plot actual vs fitted  
ggplot(data = aug_train_happy_tree, aes(x = .pred, y = BodyTemp)) +
  geom_point()

#plot residuals vs fitted 
aug_train_happy_tree %>% 
  mutate(resid = BodyTemp - .pred) %>% 
  ggplot(aes(x = .pred, y = resid)) +
  geom_point()

```

```{r lasso, cache=TRUE}
# 1) define model spec 
lasso_tune_spec <- linear_reg(
  penalty = tune(), mixture = 1) %>% 
  set_engine('glmnet')
lasso_tune_spec

# 2) workflow 
lasso_wflow <- workflow() %>% 
  add_model(lasso_tune_spec) %>% 
  add_recipe(temp_rec)


# 3) create tuning grid 
lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lasso_grid %>% top_n(-5)

lasso_grid %>% top_n(5)

# tune hyperparameters 
lasso_res <- lasso_wflow %>% 
  tune_grid(resample = cv5,
            grid = lasso_grid,
            control = control_grid(save_pred = T, verbose = T),
            metrics = metric_set(rmse))

# 4) autoplot the results 
lasso_res %>% autoplot()

#5) finalize the workflow
# select best hyperparameter combination 
best_lasso <- lasso_res %>% 
  select_best()
best_lasso

# finalize workflow 
best_lasso_wflow <- lasso_wflow %>% 
  finalize_workflow(best_lasso)

fit_best_lasso <- best_lasso_wflow %>% 
  fit(data = train_data)

#6) fit the model to the test data 
# Use a trained workflow to predict section using train data
predict(fit_best_lasso, new_data = train_data)

# include predicted probabilities
aug_best_lasso <- augment(fit_best_lasso, new_data = train_data)

# estimate model performance with rmse 
lasso_rmse <- aug_best_lasso %>% 
  rmse(truth = BodyTemp, .pred) %>% 
  mutate(model = 'LASSO')

# 8) Make diagnostic plots 
## view coefficient values vs penalty terms 
extract_fit_engine(fit_best_lasso) |> plot('lambda')

#plot actual vs fitted  
ggplot(data = aug_best_lasso, aes(x = .pred, y = BodyTemp)) +
  geom_point()

#plot residuals vs fitted 
aug_best_lasso %>% 
  mutate(resid = BodyTemp - .pred) %>% 
  ggplot(aes(x = .pred, y = resid)) +
  geom_point()
```

```{r random forest, cache=TRUE}
#cores <- parallel::detectCores()
#cores

# 1) define model spec 
forest_spec <-
  rand_forest(
    mtry = tune(),
    min_n = tune(),
    trees = 1000
    ) %>%
  set_engine('ranger', 
             num.threads = 1) %>%
  set_mode('regression')
forest_spec

# 2) workflow 
forest_wflow <- workflow() %>% 
  add_model(forest_spec) %>% 
  add_recipe(temp_rec)


# 3/4) create tuning grid/tune hyperparameters 
forest_spec %>% parameters()

forest_res <- forest_wflow %>% 
  tune_grid(resample = cv5,
            grid = 25,
            metrics = metric_set(rmse),
            control = control_grid(save_pred = T, verbose = T))


forest_res %>% show_best(metric = "rmse")


# 5) autoplot the results 
forest_res %>% autoplot()

# select best hyperparameter combination 
best_forest <- forest_res %>% 
  select_best() 
best_forest

# finalize workflow 
best_forest_wflow <- forest_wflow %>% 
  finalize_workflow(best_forest)

fit_best_forest <- best_forest_wflow %>% 
  fit(data = train_data)

# Use a trained workflow to predict section using train data
predict(fit_best_forest, new_data = train_data)

# include predicted probabilities
aug_best_forest <- augment(fit_best_forest, new_data = train_data)

# estimate model performance with rmse 
forest_rmse <- aug_best_forest %>% 
  rmse(truth = BodyTemp, .pred) %>% 
  mutate(model = 'Random Forest')

# 8) Make diagnostic plots 

#plot actual vs fitted  
ggplot(data = aug_best_forest, aes(x = .pred, y = BodyTemp)) +
  geom_point()

#plot residuals vs fitted 
aug_best_forest %>% 
  mutate(resid = BodyTemp - .pred) %>% 
  ggplot(aes(x = .pred, y = resid)) +
  geom_point()
```

#
```{r}
#view RMSEs of all four models 
knitr::kable(rbind(null_rmse, tree_rmse, lasso_rmse, forest_rmse))
```

The random forest model performed far better than the other three, according to the RMSE of 1.06, so I will fit the test data to this model.

```{r final fitting}
forest_final <- best_forest_wflow %>% 
  last_fit(data_split) 


# Use a trained workflow to predict section using test data
forest_final %>% 
  collect_predictions() 

# include predicted probabilities
aug_final_forest <- augment(forest_final)

# estimate model performance with rmse 
aug_final_forest %>% 
  rmse(truth = BodyTemp, .pred) 

# 8) Make diagnostic plots 

#plot actual vs fitted  
ggplot(data = aug_final_forest, aes(x = .pred, y = BodyTemp)) +
  geom_point()

#plot residuals vs fitted 
aug_final_forest %>% 
  mutate(resid = BodyTemp - .pred) %>% 
  ggplot(aes(x = .pred, y = resid)) +
  geom_point()
```